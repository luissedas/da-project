{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification\n",
    "Usar regresión logística con score arriba/debajo de la media para determinar cuáles son los atributos más relevantes para investigadores vs no investigadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 $\\chi^2-$test <a id='chi_test'></a>\n",
    "A chi-squared test, also written as χ2 test, is any statistical hypothesis test where the sampling distribution of the test statistic is a chi-squared distribution when the null hypothesis is true. Without other qualification, 'chi-squared test' often is used as short for Pearson's chi-squared test. The chi-squared test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2 Clustering <a id='clustering'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = data.copy()\n",
    "data_temp = data_temp.dropna(subset=['score'])\n",
    "#We create a classification variable for the performance of a professor, separating in two DF's\n",
    "cond1 = (data_temp.score >= data_temp.score.quantile(0.75)).replace([True,False],['good',''])\n",
    "cond2 = (np.logical_and(data_temp.score < data_temp.score.quantile(0.75),data_temp.score > data_temp.score.quantile(0.25))).replace([True,False],['regular',''])\n",
    "cond3 = (data_temp.score <= data_temp.score.quantile(0.25)).replace([True,False],['bad',''])\n",
    "data_temp['score_category'] = (cond1+cond2+cond3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mty = data[data.Campus =='Campus Monterrey'];\n",
    "data_mty = data_mty.dropna(subset=['score'])\n",
    "cond1 = (data_mty.score >= data_mty.score.quantile(0.75)).replace([True,False],['good',''])\n",
    "cond2 = (np.logical_and(data_mty.score < data_mty.score.quantile(0.75),data_mty.score > data_mty.score.quantile(0.25))).replace([True,False],['regular',''])\n",
    "cond3 = (data_mty.score <= data_mty.score.quantile(0.25)).replace([True,False],['bad',''])\n",
    "data_mty['score_category'] = (cond1+cond2+cond3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a classification variable for the performance of a professor, separating in two DF's\n",
    "cond1 = (df_under.score >= df_under.score.quantile(0.75)).replace([True,False],['good',''])\n",
    "cond2 = (np.logical_and(df_under.score < df_under.score.quantile(0.75),df_under.score > df_under.score.quantile(0.25))).replace([True,False],['regular',''])\n",
    "cond3 = (df_under.score <= df_under.score.quantile(0.25)).replace([True,False],['bad',''])\n",
    "df_under['score_category'] = (cond1+cond2+cond3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a classification variable for the performance of a professor, separating in two DF's\n",
    "cond1 = (df_grad.score >= df_grad.score.quantile(0.75)).replace([True,False],['good',''])\n",
    "cond2 = (np.logical_and(df_grad.score < df_grad.score.quantile(0.75),df_grad.score > df_grad.score.quantile(0.25))).replace([True,False],['regular',''])\n",
    "cond3 = (df_grad.score <= df_grad.score.quantile(0.25)).replace([True,False],['bad',''])\n",
    "df_grad['score_category'] = (cond1+cond2+cond3)\n",
    "df_grad = df_grad[df_grad.score_category != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic regression <a id='logistic_regression'></a>\n",
    "\n",
    "1. **<span style=\"color:green\">✓</span>** Binary logistic regression requires the dependent variable to be binary. \n",
    "2. **<span style=\"color:green\">✓</span>** For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n",
    "3. Only the meaningful variables should be included.\n",
    "4. The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n",
    "5. The independent variables are linearly related to the log odds.\n",
    "6. Logistic regression requires quite large sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "#https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Decision Tree <a id='decision_tree'></a>\n",
    "Pros\n",
    "- Decision trees are easy to interpret and visualize.\n",
    "- It can easily capture Non-linear patterns.\n",
    "- It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n",
    "- It can be used for feature engineering such as predicting missing values, suitable for variable selection.\n",
    "- The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm. (Source)\n",
    "\n",
    "Cons\n",
    "- Sensitive to noisy data. It can overfit noisy data.\n",
    "- The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n",
    "- Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "#https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graduate <a id='decision_tree_grad'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad_temp = df_grad[['score_category','sni_yn','under_yn','experience','grado']].dropna()\n",
    "df_grad_temp_X = df_grad_temp.drop('score_category',axis=1)\n",
    "df_grad_temp_y = df_grad_temp.score_category\n",
    "\n",
    "# Encodes catagerical data\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = df_grad_temp_X.values\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(df_grad_temp_X.values);\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.ndarray.tolist(df_grad_temp_y.unique()))\n",
    "y_encoded = le.transform(df_grad_temp_y);\n",
    "\n",
    "#Splits the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing state\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draws the decision tree\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf,\n",
    "                                out_file=None,\n",
    "                                feature_names = np.ndarray.tolist(df_grad_temp_X.columns.values),\n",
    "                                class_names = df_grad_temp_y.unique(),\n",
    "                                filled=True,\n",
    "                                rounded=True,\n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "#graph.render('tree_grad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undergraduate <a id='decision_tree_under'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_under_temp = df_under[['score_category','sni_yn','under_yn','experience','grado']].dropna()\n",
    "df_under_temp_X = df_under_temp.drop('score_category',axis=1)\n",
    "df_under_temp_y = df_under_temp.score_category\n",
    "\n",
    "# Encodes catagerical data\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = df_under_temp_X.values\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(df_under_temp_X.values);\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.ndarray.tolist(df_under_temp_y.unique()))\n",
    "y_encoded = le.transform(df_under_temp_y);\n",
    "\n",
    "#Splits the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing state\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp2 = data_temp[['score_category','sni_yn','under_yn','experience','grado']].dropna()\n",
    "data_temp2_X = data_temp2.drop('score_category',axis=1)\n",
    "data_temp2_y = data_temp2.score_category\n",
    "\n",
    "# Encodes catagerical data\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = data_temp2_X.values\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(data_temp2_X.values);\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.ndarray.tolist(data_temp2_y.unique()))\n",
    "y_encoded = le.transform(data_temp2_y);\n",
    "\n",
    "#Splits the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing state\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Campus Mty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mty_temp = data_mty[['score_category','sni_yn','under_yn','experience','grado']].dropna()\n",
    "df_mty_temp_X = df_mty_temp.drop('score_category',axis=1)\n",
    "df_mty_temp_y = df_mty_temp.score_category\n",
    "\n",
    "# Encodes catagerical data\n",
    "enc = preprocessing.OrdinalEncoder()\n",
    "X = df_mty_temp_X.values\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(df_mty_temp_X.values);\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.ndarray.tolist(df_mty_temp_y.unique()))\n",
    "y_encoded = le.transform(df_mty_temp_y);\n",
    "\n",
    "#Splits the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing state\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
